{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets as tf_datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    y_onehot = np.zeros([len(y), len(np.unique(y))])\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        y_onehot[i, y[i]] = 1\n",
    "        \n",
    "    return y_onehot\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf_datasets.mnist.load_data()\n",
    "\n",
    "training_rows = 10000\n",
    "testing_rows = 1000\n",
    "\n",
    "X_train = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])[0:training_rows, :]\n",
    "X_test = x_test.reshape(x_test.shape[0], x_train.shape[1] * x_train.shape[2])[0:testing_rows, :]\n",
    "\n",
    "y_train = y_train[0:training_rows]\n",
    "y_test = y_test[0:testing_rows]\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "y_train_oh = one_hot(y_train)[0:training_rows, :]\n",
    "y_test_oh = one_hot(y_test)[0:testing_rows, :]\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(y_train)):\n",
    "    train_data.append((X_train[i, :], y_train[i]))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=784, bias=True)\n",
      "  (fc2): Linear(in_features=784, out_features=80, bias=True)\n",
      "  (fc3): Linear(in_features=80, out_features=20, bias=True)\n",
      "  (fc4): Linear(in_features=20, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 784)\n",
    "        self.fc2 = nn.Linear(784, 80)\n",
    "        self.fc3 = nn.Linear(80, 20)\n",
    "        self.fc4 = nn.Linear(20, 10)\n",
    "        \n",
    "    def Forward(self, X):\n",
    "        X = torch.tanh(self.fc1(X))\n",
    "        X = torch.tanh(self.fc2(X))\n",
    "        X = torch.tanh(self.fc3(X))\n",
    "        X = self.fc4(X)\n",
    "        return X\n",
    "    \n",
    "net = Net()\n",
    "net = net.float()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = .0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[292,  2000] loss: 0.004\n",
      "[293,  2000] loss: 0.004\n",
      "[294,  2000] loss: 0.004\n",
      "[295,  2000] loss: 0.004\n",
      "[296,  2000] loss: 0.004\n",
      "[297,  2000] loss: 0.004\n",
      "[298,  2000] loss: 0.004\n",
      "[299,  2000] loss: 0.004\n",
      "[300,  2000] loss: 0.004\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(300):\n",
    "    running_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net.Forward(inputs.float())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            losses.append(running_loss)\n",
    "            running_loss = 0\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9340000152587891\n"
     ]
    }
   ],
   "source": [
    "preds = net.Forward(torch.from_numpy(X_test).float()).max(1)[1]\n",
    "print(torch.mean((preds == torch.from_numpy(y_test).long()).float()).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
