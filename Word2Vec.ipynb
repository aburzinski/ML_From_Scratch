{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r93-rhaghKMj",
        "colab_type": "text"
      },
      "source": [
        "### Import Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HZG77tAhCaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-gvIQUrhQI4",
        "colab_type": "text"
      },
      "source": [
        "### Create Required Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxw5VCZehadt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(v):\n",
        "    exps = np.exp(v)\n",
        "    return exps / np.sum(exps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfXfbLAxhoPg",
        "colab_type": "text"
      },
      "source": [
        "### Create Skip-Gram Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPCGheaqhrrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2Vec:\n",
        "    \n",
        "    def __init__(self, settings):\n",
        "        # n is hidden node size\n",
        "        self.n = settings['n']\n",
        "        \n",
        "        self.eta = settings['learning_rate']\n",
        "        self.epochs = settings['epochs']\n",
        "        self.window_size = settings['window_size']\n",
        "        \n",
        "    def onehot(self, word):\n",
        "        vec = [0 for _ in range(len(self.vocab))]\n",
        "        vec[self.word_index[word]] = 1\n",
        "        return vec\n",
        "        \n",
        "    def generate_training_data(self, settings, corpus):\n",
        "        \n",
        "        word_counts = defaultdict(int)\n",
        "        for row in corpus:\n",
        "            for word in row:\n",
        "                word_counts[word] += 1\n",
        "                \n",
        "        self.vocab = sorted(list(word_counts.keys()), reverse = False)\n",
        "        self.word_index = {word: i for i, word in enumerate(self.vocab)}\n",
        "        self.index_word = {i: word for i, word in enumerate(self.vocab)}\n",
        "        \n",
        "        # V is vocab size\n",
        "        self.V = len(self.vocab)\n",
        "        \n",
        "        training_data = []\n",
        "        for sentence in corpus:\n",
        "            sentence_length = len(sentence)\n",
        "            \n",
        "            for i, word in enumerate(sentence):\n",
        "                \n",
        "                x_t = self.onehot(word)\n",
        "                \n",
        "                x_context = []\n",
        "                for j in range(i - self.window_size, i + self.window_size + 1):\n",
        "                    if j != i and j >= 0 and j < sentence_length:\n",
        "                        x_context.append(self.onehot(sentence[j]))\n",
        "                \n",
        "                training_data.append([x_t, x_context])\n",
        "        return np.array(training_data)\n",
        "        \n",
        "    def forward_pass(self, x):\n",
        "        h = np.dot(np.transpose(self.W1), x)\n",
        "        u_c = np.dot(np.transpose(self.W2), h)\n",
        "        y_c = softmax(u_c)\n",
        "        return y_c, h, u_c\n",
        "    \n",
        "    def back_prop(self, e, h, x):\n",
        "        dl_dW2 = np.outer(h, e)\n",
        "        dl_dW1 = np.outer(x, np.dot(self.W2, e.reshape(-1, 1)))\n",
        "        \n",
        "        self.W2 = self.W2 - self.eta * dl_dW2\n",
        "        self.W1 = self.W1 - self.eta * dl_dW1\n",
        "        pass\n",
        "    \n",
        "    def train(self, training_data):\n",
        "        self.W1 = np.random.uniform(-1, 1, (self.V, self.n))\n",
        "        self.W2 = np.random.uniform(-1, 1, (self.n, self.V))\n",
        "        \n",
        "        for i in range(self.epochs):\n",
        "            self.loss = 0\n",
        "            \n",
        "            for x_t, x_c in training_data:\n",
        "                y_pred, h, u = self.forward_pass(x_t)\n",
        "                \n",
        "                EI = np.sum([np.subtract(y_pred, word) for word in x_c], axis=0)\n",
        "                \n",
        "                self.back_prop(EI, h, x_t)\n",
        "                \n",
        "                self.loss += -np.sum([np.log(np.dot(y_pred, word)) for word in x_c])\n",
        "            \n",
        "            if i % (self.epochs / 10) == 0: print('Epoch: ' + str(i) + ' Loss: ' + str(self.loss))\n",
        "        pass\n",
        "    \n",
        "    def get_word_vec(self, word):\n",
        "        word_idx = self.word_index[word]\n",
        "        return self.W1[word_idx, :]\n",
        "    \n",
        "    def word_sim(self, word, top_n = 5):\n",
        "        word_vec = self.get_word_vec(word)\n",
        "        \n",
        "        word_sim = {}\n",
        "        for vocab_word in self.word_index:\n",
        "            if word != vocab_word:\n",
        "                vec_w2 = self.get_word_vec(vocab_word)\n",
        "                num = np.dot(word_vec, vec_w2)\n",
        "                denom = np.linalg.norm(word_vec) * np.linalg.norm(vec_w2)\n",
        "                word_sim[vocab_word] = num / denom\n",
        "                \n",
        "        words_sorted = sorted(word_sim.items(), key = lambda item: item[1], reverse = True)\n",
        "        \n",
        "        for word, sim in words_sorted[:top_n]:\n",
        "            print(word, sim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RZy-QLJkUEn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "0e20df6f-b07f-4474-a13a-542fd34d6aa1"
      },
      "source": [
        "corpus = [['this', 'is', 'the', 'first', 'sentence'],\n",
        "          ['this', 'is', 'the', 'second', 'sentence'],\n",
        "          ['the', 'third', 'sentence', 'is', 'different', 'from', 'the', 'first', 'two'],\n",
        "          ['so', 'is', 'the', 'fourth']]\n",
        "\n",
        "\n",
        "settings = {'n': 10, 'learning_rate': .01, 'epochs': 2000, 'window_size': 2}\n",
        "w2v = Word2Vec(settings)\n",
        "\n",
        "training_data = w2v.generate_training_data(settings, corpus)\n",
        "\n",
        "w2v.train(training_data)\n"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Loss: 196.14718056660485\n",
            "Epoch: 200 Loss: 116.83386586821497\n",
            "Epoch: 400 Loss: 116.15797649524616\n",
            "Epoch: 600 Loss: 116.01218234845193\n",
            "Epoch: 800 Loss: 115.94068324917843\n",
            "Epoch: 1000 Loss: 115.89182065651333\n",
            "Epoch: 1200 Loss: 115.85329889158156\n",
            "Epoch: 1400 Loss: 115.8209280241729\n",
            "Epoch: 1600 Loss: 115.79286460959213\n",
            "Epoch: 1800 Loss: 115.76811224422065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lLOBNkk4nv3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "cc46150e-bdbc-437d-9481-fee5b87d31ae"
      },
      "source": [
        "w2v.word_sim('second', 5)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "third 0.9093000152003493\n",
            "different 0.5868957135620084\n",
            "fourth 0.5644147557840082\n",
            "this 0.5431390836968107\n",
            "so 0.5047346072149316\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}